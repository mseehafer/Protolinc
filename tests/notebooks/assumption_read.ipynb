{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import nan\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b990b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sheetnames(sheet_names):\n",
    "    \"\"\" Identify which data to read from which sheet. \"\"\"\n",
    "    sheet_data = []\n",
    "    for sheetname in sheet_names:\n",
    "        sheetname = sheetname.strip().upper()\n",
    "        # check that there are brackes\n",
    "        pos1 = sheetname.find(\"(\")\n",
    "        pos2 = sheetname.find(\")\")\n",
    "        if pos1 <= 0 or pos2 <= 0:\n",
    "            print(\"Skipping sheet\", sheetname, \"(no brackets found)\")\n",
    "            continue\n",
    "\n",
    "        transition_name = sheetname[:pos1].strip()\n",
    "        transition = sheetname[pos1+1:pos2]\n",
    "        pos_arrow = transition.find(\"->\")\n",
    "        if pos_arrow <= 0:\n",
    "            print(\"Skipping sheet\", sheetname, \"(no arrow found)\")\n",
    "            continue\n",
    "    \n",
    "        from_state = int(transition[:pos_arrow].strip())\n",
    "        to_state = int(transition[pos_arrow+2:].strip())\n",
    "        \n",
    "        print(\"Sheet\", sheetname, \"describes the transition\", from_state, \"==>\", to_state)\n",
    "        sheet_data.append((sheetname, transition_name, from_state, to_state))\n",
    "        \n",
    "    return sheet_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAssumption:\n",
    "    \"\"\" Class to represent a core assumption table as read in from Excel.\"\"\"\n",
    "    \n",
    "    def __init__(self, sheet_data, values, vert_rf, horz_rf, vert_attr_values, horz_attr_values):\n",
    "        self.values = values\n",
    "        self.vert_rf = vert_rf\n",
    "        self.horz_rf = horz_rf\n",
    "        self.vert_attr_values = vert_attr_values\n",
    "        self.horz_attr_values = horz_attr_values\n",
    "        self.transition_name = sheet_data[1]\n",
    "        self.from_state_num = sheet_data[2]\n",
    "        self.to_state_num = sheet_data[3]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<BaseAssumption: {}>\".format(self.transition_name)\n",
    "\n",
    "def generate_zero_transition(from_state_num, to_state_num):\n",
    "    \"\"\" Helper method to avoid dealing with case\n",
    "        distinction for implicitly impossible transitions. \"\"\"\n",
    "    tr_name = 'Implicit Impossible Transition ({}->{})'.format(from_state_num, to_state_num)\n",
    "    sheet_data = (tr_name, tr_name, from_state_num, to_state_num)\n",
    "    df_table = pd.DataFrame({nan: [0.0]}, index=[nan])\n",
    "    vert_rf = None\n",
    "    horz_rf = None,\n",
    "    vert_attr_values = [nan]\n",
    "    horz_attr_values = [nan]\n",
    "\n",
    "    return BaseAssumption(sheet_data, df_table, vert_rf, horz_rf, vert_attr_values, horz_attr_values)\n",
    "\n",
    "\n",
    "def read_sheet(xl_file, sheet_data):\n",
    "    \"\"\" Create a BaseAssumption object from the data in the sheet.\"\"\"\n",
    "    sheet_name = sheet_data[0]\n",
    "    print(\"Processing sheet\", sheet_name)\n",
    "    df = pd.read_excel(xl_file, sheet_name= sheet_name, header=None)\n",
    "    \n",
    "    if df.loc[3, 0].upper() != \"TABLE\":\n",
    "        raise Exception(\"Invalid format\")\n",
    "    if df.loc[0, 0].upper() != 'VERTICAL_RISK_FACTOR':\n",
    "        raise Exception(\"Invalid format\")\n",
    "    if df.loc[1, 0].upper() != 'HORIZ_RISK_FACTOR':\n",
    "        raise Exception(\"Invalid format\")\n",
    "    \n",
    "    # read risk factor names\n",
    "    vert_rf = df.loc[0, 1].upper()\n",
    "    horz_rf = df.loc[1, 1].upper()\n",
    "    \n",
    "    vert_rf = None if vert_rf == 'NONE' else vert_rf\n",
    "    vert_rf_for_headers = \"DUMMY_FOR_NONE\" if vert_rf is None else vert_rf\n",
    "    horz_rf = None if horz_rf == 'NONE' else horz_rf\n",
    "    \n",
    "    # extract horizontal/vertical indexes headers from 3rd row\n",
    "    vert_attr_values = list(df.loc[4:, 0])\n",
    "    horz_attr_values = list(df.loc[3:3].values.reshape((df.shape[1],))[1:])\n",
    "    \n",
    "    df_table = df[4:].copy()\n",
    "    # df_table.columns = np.array([vert_rf] + list(horz_attr_values), dtype='object')\n",
    "    df_table.columns = pd.Series([vert_rf_for_headers] + list(horz_attr_values), dtype=\"object\")\n",
    "    print(df_table.columns, [vert_rf] + list(horz_attr_values))\n",
    "    df_table = df_table.set_index(vert_rf_for_headers)\n",
    "    return BaseAssumption(sheet_data, df_table, vert_rf, horz_rf, vert_attr_values, horz_attr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for testing\n",
    "\n",
    "path = \"../data/assumptions/base_assumption.xlsx\"\n",
    "base_assumptions = []\n",
    "with pd.ExcelFile(path) as inp:\n",
    "    sheet_data = analyze_sheetnames(inp.sheet_names)\n",
    "\n",
    "    for sd in sheet_data:\n",
    "        base_assumpt = read_sheet(inp, sd)\n",
    "        base_assumptions.append(base_assumpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ff103",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([\"TEXT\", nan], dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/assumptions/base_assumption.xlsx\"\n",
    "base_assumptions = []\n",
    "with pd.ExcelFile(path) as inp:\n",
    "    sheet_data = analyze_sheetnames(inp.sheet_names)\n",
    "\n",
    "    for sd in sheet_data:\n",
    "        try:\n",
    "            base_assumpt = read_sheet(inp, sd)\n",
    "            base_assumptions.append(base_assumpt)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping sheet\", sd[0], e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ad950",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16968812",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = base_assumptions[0]\n",
    "ba.horz_rf, ba.horz_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ab94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba.df_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4046640",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba.vert_attr_values\n",
    "ba.horz_attr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fee58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum, unique\n",
    "from commons import check_states\n",
    "\n",
    "\n",
    "@unique\n",
    "class RiskFactor(IntEnum):\n",
    "\n",
    "    def is_applicable(self, base_assumption):\n",
    "        _name = self.__class__.__name__.upper()\n",
    "        if base_assumption.horz_rf == _name or base_assumption.vert_rf == _name:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def validate_assumptions(self, base_assumptions):\n",
    "        raise Exception(\"Method must be implemented in subclass\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"RiskFactor:{}\".format(self.__class__.__name__.upper())\n",
    "\n",
    "\n",
    "@unique\n",
    "class Gender(RiskFactor):\n",
    "    M = 0\n",
    "    F = 1\n",
    "\n",
    "    def validate_assumptions(self, base_assumption):\n",
    "        if base_assumption.horz_rf == \"GENDER\":\n",
    "            self._validate_horizontal(base_assumption)\n",
    "        elif base_assumption.vert_rf == \"GENDER\":\n",
    "            base_assumption.df_table = base_assumption.df_table.T\n",
    "            self._validate_horizontal(base_assumption)\n",
    "            base_assumption.df_table = base_assumption.df_table.T\n",
    "\n",
    "    def _validate_horizontal(self, base_assumption):\n",
    "        required_rf = [g.name for g in Gender]\n",
    "        if set(base_assumption.df_table.columns) != set(required_rf):\n",
    "            raise Exception(\"Risk Factors for Gender must be \" + str({g for g in Gender}))\n",
    "        # enforce column ordering\n",
    "        base_assumption.df_table = base_assumption.df_table[required_rf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85eb3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "006a2c54",
   "metadata": {},
   "source": [
    "### Base Assumptions to Assumption Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from state import States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839da13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee617e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check states\n",
    "# find min/max and check everything in between is filled\n",
    "max_val = -1\n",
    "min_val = 999999\n",
    "state_vals = set()\n",
    "for st in States:\n",
    "    print(st, int(st))\n",
    "    max_val = max(int(st), max_val)\n",
    "    min_val = min(int(st), min_val)\n",
    "    state_vals.add(int(st))\n",
    "\n",
    "assert len(States) > 0\n",
    "assert min_val == 0\n",
    "assert max_val == len(States) - 1\n",
    "assert len(state_vals) == len(States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd54b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_assumptions_map_tmp = {(ba.from_state_num, ba.to_state_num): ba for ba in base_assumptions}\n",
    "\n",
    "# enumerate all state transitions\n",
    "transition_assumptions = []\n",
    "for i in range(max_val + 1):\n",
    "    this_list = []\n",
    "    transition_assumptions.append(this_list)\n",
    "    for j in range(max_val + 1):\n",
    "        # print(\"BEF\", i, j, transition_assumptions[i][j])\n",
    "        ts = None\n",
    "        if i != j:\n",
    "            ts = base_assumptions_map_tmp.get((i, j))\n",
    "            #print(i, j, ts)\n",
    "            if ts is None:\n",
    "                #print(\"No assumptions provided for ({i},{j}), assuming impossibility\".format(i=i, j=j))\n",
    "                ts = generate_zero_transition(i, j)\n",
    "        else:\n",
    "            # use None on the diagonal - done through the initialization\n",
    "            ts = None\n",
    "        this_list.append((ts, i, j))\n",
    "        #print(\"AFT\",i, j, transition_assumptions[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = transition_assumptions[2][3][2]\n",
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.horz_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac0667",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7289e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protolinc.assumptions import _read_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e15734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6982: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(sequences[0], name=names)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseAssumption' object has no attribute 'df_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprogramming\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpy\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPyMultiState\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprotolinc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124massumptions\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbase_assumption.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43m_read_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDIS1 (0->1)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDIS1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\programming\\py\\pymultistate\\protolinc\\assumptions.py:152\u001b[0m, in \u001b[0;36m_read_sheet\u001b[1;34m(xl_file, sheet_data)\u001b[0m\n\u001b[0;32m    150\u001b[0m df_table\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [vert_rf_for_header] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(horz_attr_values)\n\u001b[0;32m    151\u001b[0m df_table \u001b[38;5;241m=\u001b[39m df_table\u001b[38;5;241m.\u001b[39mset_index(vert_rf_for_header)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseAssumption\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvert_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorz_rf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvert_attr_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorz_attr_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\programming\\py\\pymultistate\\protolinc\\assumptions.py:78\u001b[0m, in \u001b[0;36mBaseAssumption.__init__\u001b[1;34m(self, sheet_data, values, vert_rf, horz_rf, vert_attr_values, horz_attr_values, is_zero)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rf\u001b[38;5;241m.\u001b[39mis_applicable(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     77\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m applicable for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, rf, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_assumptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\programming\\py\\pymultistate\\protolinc\\risk_factors.py:56\u001b[0m, in \u001b[0;36mAge.validate_assumptions\u001b[1;34m(cls, base_assumption)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m base_assumption\u001b[38;5;241m.\u001b[39mvert_rf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     55\u001b[0m     base_assumption\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m base_assumption\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_horizontal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_assumption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     base_assumption\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m base_assumption\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[1;32md:\\programming\\py\\pymultistate\\protolinc\\risk_factors.py:62\u001b[0m, in \u001b[0;36mAge._validate_horizontal\u001b[1;34m(cls, base_assumption)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_horizontal\u001b[39m(\u001b[38;5;28mcls\u001b[39m, base_assumption):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# for the age we require that the index are integers from zero to at least 119\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mbase_assumption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_table\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexes) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m120\u001b[39m:\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRisk Factor AGE must have at least 120 entries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseAssumption' object has no attribute 'df_table'"
     ]
    }
   ],
   "source": [
    "path = r\"D:\\programming\\py\\PyMultiState\\protolinc\\data\\assumptions\\base_assumption.xlsx\"\n",
    "_read_sheet(path, (\"DIS1 (0->1)\", \"DIS1\", 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2]).reshape((1, 2)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d3419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
